{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutoriel : interagir avec le système de stockage S3 du SSP Cloud (MinIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import s3fs\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Récupérer les données d'un challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create filesystem object\n",
    "S3_ENDPOINT_URL = \"https://\" + os.environ[\"AWS_S3_ENDPOINT\"]\n",
    "fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': S3_ENDPOINT_URL})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gvimont/diffusion/hackathon-minarm-2024/AIVSAI',\n",
       " 'gvimont/diffusion/hackathon-minarm-2024/Acoustique',\n",
       " 'gvimont/diffusion/hackathon-minarm-2024/Similarité']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lister les challenges\n",
    "fs.ls(\"gvimont/diffusion/hackathon-minarm-2024\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gvimont/diffusion/hackathon-minarm-2024/Similarité/.keep',\n",
       " 'gvimont/diffusion/hackathon-minarm-2024/Similarité/archive.zip']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lister les fichiers d'un challenge\n",
    "fs.ls(\"gvimont/diffusion/hackathon-minarm-2024/Similarité\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Télécharger les données dans le service\n",
    "PATH_IN = 'gvimont/diffusion/hackathon-minarm-2024/Similarité/archive.zip'\n",
    "fs.download(PATH_IN, 'data/archive.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Décompresser les données\n",
    "with zipfile.ZipFile(\"data/archive.zip\",\"r\") as zip_file:\n",
    "    zip_file.extractall(\"data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB : les données peuvent être également téléchargées directement si besoin, pour être utilisées hors du SSP CLoud.\n",
    "Exemple pour le fichier ci-dessus (même format de lien pour les autres challenges) : \n",
    "\n",
    "http://minio.lab.sspcloud.fr/gvimont/diffusion/hackathon-minarm-2024/AIVSAI/HC3.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporter des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Imagefile</th>\n",
       "      <th>Bounding_boxe1</th>\n",
       "      <th>Bounding_boxe2</th>\n",
       "      <th>Bounding_boxe3</th>\n",
       "      <th>Bounding_boxe4</th>\n",
       "      <th>class number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001.jpg</td>\n",
       "      <td>39</td>\n",
       "      <td>116</td>\n",
       "      <td>569</td>\n",
       "      <td>375</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00002.jpg</td>\n",
       "      <td>36</td>\n",
       "      <td>116</td>\n",
       "      <td>868</td>\n",
       "      <td>587</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00003.jpg</td>\n",
       "      <td>85</td>\n",
       "      <td>109</td>\n",
       "      <td>601</td>\n",
       "      <td>381</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00004.jpg</td>\n",
       "      <td>621</td>\n",
       "      <td>393</td>\n",
       "      <td>1484</td>\n",
       "      <td>1096</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00005.jpg</td>\n",
       "      <td>14</td>\n",
       "      <td>36</td>\n",
       "      <td>133</td>\n",
       "      <td>99</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Imagefile  Bounding_boxe1  Bounding_boxe2  Bounding_boxe3  Bounding_boxe4  \\\n",
       "0  00001.jpg              39             116             569             375   \n",
       "1  00002.jpg              36             116             868             587   \n",
       "2  00003.jpg              85             109             601             381   \n",
       "3  00004.jpg             621             393            1484            1096   \n",
       "4  00005.jpg              14              36             133              99   \n",
       "\n",
       "   class number  \n",
       "0            14  \n",
       "1             3  \n",
       "2            91  \n",
       "3           134  \n",
       "4           106  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#df = pd.read.('/data/HC3/medicine.jsonl', lines=True)\n",
    "df_train = pd.read_csv(r\"./data/anno_train.csv\", header=None)\n",
    "df_test = pd.read_csv(r\"./data/anno_test.csv\", header=None)\n",
    "\n",
    "# Ajouter des en-têtes au DataFrame\n",
    "headers = ['Imagefile', 'Bounding_boxe1','Bounding_boxe2' ,'Bounding_boxe3','Bounding_boxe4','class number']\n",
    "df_train.columns = headers\n",
    "df_test.columns = headers\n",
    "\n",
    "\n",
    "df_train.head()\n",
    "#df_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AM General Hummer SUV 2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Acura RL Sedan 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Acura TL Sedan 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Acura TL Type-S 2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Acura TSX Sedan 2012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Name\n",
       "0  AM General Hummer SUV 2000\n",
       "1         Acura RL Sedan 2012\n",
       "2         Acura TL Sedan 2012\n",
       "3        Acura TL Type-S 2008\n",
       "4        Acura TSX Sedan 2012"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_names = pd.read_csv(r\"./data/names.csv\", header=None)\n",
    "\n",
    "headers_names = [\"Name\"]\n",
    "\n",
    "df_names.columns = headers_names\n",
    "\n",
    "df_names.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Imagefile         0\n",
       "Bounding_boxe1    0\n",
       "Bounding_boxe2    0\n",
       "Bounding_boxe3    0\n",
       "Bounding_boxe4    0\n",
       "class number      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class CarsDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            annotations_file (str): Chemin vers le fichier qui contient les annotations.\n",
    "            img_dir (str): Chemin du répertoire contenant les images.\n",
    "            transform (callable, optional): Transformations optionnelles à appliquer sur les images.\n",
    "        \"\"\"\n",
    "        self.img_labels = pd.read_csv(annotations_file, header=None)\n",
    "        headers = ['Imagefile', 'Bounding_boxe1','Bounding_boxe2' ,'Bounding_boxe3','Bounding_boxe4','class number']\n",
    "        self.img_labels.columns = headers\n",
    "        self.img_labels['class number'] -= 1  # Adjust labels to be zero-indexed\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.img_labels.iloc[idx, 5]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Chemins vers vos fichiers CSV et dossiers d'images\n",
    "train_file = \"data/anno_train.csv\"\n",
    "test_file = \"data/anno_test.csv\"\n",
    "train_dir = \"data/cars_train\"\n",
    "test_dir = \"data/cars_test\"\n",
    "\n",
    "# Créer les datasets\n",
    "train_dataset = CarsDataset(train_file, train_dir, transform)\n",
    "test_dataset = CarsDataset(test_file, test_dir, transform)\n",
    "\n",
    "# Créer les DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/mamba/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = resnet18(pretrained=True)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 196)  # 196 est le nombre de classes dans votre cas\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 3.947674258550008\n",
      "Accuracy of the model on test images: 10.695187165775401%\n",
      "Epoch [2/10], Loss: 2.859018172002306\n",
      "Accuracy of the model on test images: 26.501678895659744%\n",
      "Epoch [3/10], Loss: 1.9992124866036807\n",
      "Accuracy of the model on test images: 44.173610247481655%\n",
      "Epoch [4/10], Loss: 1.391171489977369\n",
      "Accuracy of the model on test images: 53.475935828877006%\n",
      "Epoch [5/10], Loss: 1.0471586209886214\n",
      "Accuracy of the model on test images: 52.76706877254073%\n",
      "Epoch [6/10], Loss: 0.7942011890458126\n",
      "Accuracy of the model on test images: 58.33851511006094%\n",
      "Epoch [7/10], Loss: 0.5935275519011067\n",
      "Accuracy of the model on test images: 59.830866807610995%\n",
      "Epoch [8/10], Loss: 0.4921063833376941\n",
      "Accuracy of the model on test images: 56.43576669568461%\n",
      "Epoch [9/10], Loss: 0.3910020934600456\n",
      "Accuracy of the model on test images: 66.683248352195%\n",
      "Epoch [10/10], Loss: 0.37921167074465284\n",
      "Accuracy of the model on test images: 63.78559880611864%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader)}')\n",
    "\n",
    "    # Evaluation on the test set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        print(f'Accuracy of the model on test images: {100 * correct / total}%')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
